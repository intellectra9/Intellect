# .github/workflows/generate_transcriptions.yml
name: Generate Sentence Transcriptions

on:
  push:
    branches: [ main ]
    paths:
      - 'Visuals/visuals.json'
      - 'Trans/transcription.json'
  workflow_dispatch:

jobs:
  generate-transcriptions:
    runs-on: ubuntu-latest
    
    # Add permissions for GITHUB_TOKEN
    permissions:
      contents: write
      actions: read
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
        
    - name: Pull latest changes
      run: |
        git config --local user.name "intellectra9"
        git config --local user.email "intellectra9@outlook.com"
        git pull origin main --rebase || git pull origin main
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Generate sentence transcriptions
      run: |
        python3 << 'EOF'
        import json
        import os
        import re
        
        def load_json_file(file_path):
            """Load JSON file and return data"""
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    return json.load(file)
            except FileNotFoundError:
                print(f"Error: File {file_path} not found")
                return None
            except json.JSONDecodeError:
                print(f"Error: Invalid JSON in {file_path}")
                return None
        
        def clean_word(word):
            """Remove punctuation and convert to lowercase for matching"""
            return re.sub(r'[^\w\s]', '', word.lower().strip())
        
        def find_sentence_in_sequence(sentence_words, word_segments, start_index=0):
            """Find sentence words sequentially in transcription starting from start_index"""
            if not sentence_words or start_index >= len(word_segments):
                return -1, -1, start_index
            
            current_index = start_index
            matched_words = 0
            sentence_start_index = -1
            sentence_end_index = -1
            
            # Look for the first word of the sentence
            while current_index < len(word_segments):
                word_cleaned = clean_word(word_segments[current_index]['word'])
                
                if word_cleaned == sentence_words[0]:
                    # Found potential start, check if we can match the full sentence
                    sentence_start_index = current_index
                    temp_index = current_index
                    matched_words = 0
                    
                    for sentence_word in sentence_words:
                        # Look for this word within a reasonable range
                        found = False
                        search_range = min(temp_index + 5, len(word_segments))  # Look ahead max 5 words
                        
                        for search_idx in range(temp_index, search_range):
                            if search_idx >= len(word_segments):
                                break
                            search_word_cleaned = clean_word(word_segments[search_idx]['word'])
                            
                            if search_word_cleaned == sentence_word:
                                matched_words += 1
                                sentence_end_index = search_idx
                                temp_index = search_idx + 1
                                found = True
                                break
                        
                        if not found:
                            break
                    
                    # Check if we matched enough words (at least 70% of the sentence)
                    match_ratio = matched_words / len(sentence_words)
                    if match_ratio >= 0.7:
                        return sentence_start_index, sentence_end_index, sentence_end_index + 1
                
                current_index += 1
            
            return -1, -1, current_index
        
        def create_sentence_transcription(visuals_data, transcription_data):
            """Create sentence-level transcription sequentially"""
            sentence_transcriptions = []
            word_segments = transcription_data.get('word_segments', [])
            
            if not word_segments:
                print("Error: No word_segments found in transcription data")
                return []
            
            current_word_index = 0  # Track position in word sequence
            
            for item in visuals_data:
                sentence = item.get('sentence', '')
                sentence_number = item.get('number', 0)
                
                if not sentence:
                    print(f"Warning: Empty sentence for item {sentence_number}")
                    continue
                
                # Split sentence into words and clean them
                sentence_words = [clean_word(word) for word in sentence.split() if clean_word(word)]
                
                if not sentence_words:
                    print(f"Warning: No valid words in sentence {sentence_number}")
                    continue
                
                # Find this sentence starting from current position
                start_idx, end_idx, next_start = find_sentence_in_sequence(
                    sentence_words, word_segments, current_word_index
                )
                
                if start_idx >= 0 and end_idx >= 0:
                    start_time = word_segments[start_idx]['start']
                    end_time = word_segments[end_idx]['end']
                    
                    transcription_item = {
                        "id": f"i{sentence_number}",
                        "number": sentence_number,
                        "sentence": sentence,
                        "start": start_time,
                        "end": end_time,
                        "duration": round(end_time - start_time, 3)
                    }
                    sentence_transcriptions.append(transcription_item)
                    current_word_index = next_start  # Move to next position
                    
                    print(f"‚úì Sentence {sentence_number}: {start_time}s - {end_time}s")
                else:
                    print(f"‚úó Could not find sentence {sentence_number} in transcription")
                    # Add with null timestamps but don't advance word index
                    transcription_item = {
                        "id": f"i{sentence_number}",
                        "number": sentence_number,
                        "sentence": sentence,
                        "start": None,
                        "end": None,
                        "duration": None
                    }
                    sentence_transcriptions.append(transcription_item)
            
            return sentence_transcriptions
        
        def main():
            """Main function to process files and create sentence-level transcription"""
            # File paths
            visuals_file = "Visuals/visuals.json"
            transcription_file = "Trans/transcription.json"
            output_file = "Edits/edit.json"
            
            # Create Edits directory if it doesn't exist
            os.makedirs("Edits", exist_ok=True)
            
            # Load data files
            print("Loading visuals data...")
            visuals_data = load_json_file(visuals_file)
            if not visuals_data:
                print("Failed to load visuals data")
                exit(1)
            
            print("Loading transcription data...")
            transcription_data = load_json_file(transcription_file)
            if not transcription_data:
                print("Failed to load transcription data")
                exit(1)
            
            print(f"Processing {len(visuals_data)} sentences sequentially...")
            
            # Create sentence-level transcriptions
            sentence_transcriptions = create_sentence_transcription(visuals_data, transcription_data)
            
            if sentence_transcriptions:
                # Save results
                output_data = {
                    "sentence_transcriptions": sentence_transcriptions,
                    "total_sentences": len(sentence_transcriptions),
                    "successful_matches": len([s for s in sentence_transcriptions if s['start'] is not None]),
                    "metadata": {
                        "generated_by": "sequential_sentence_transcription_generator",
                        "source_files": {
                            "visuals": visuals_file,
                            "transcription": transcription_file
                        }
                    }
                }
                
                try:
                    with open(output_file, 'w', encoding='utf-8') as file:
                        json.dump(output_data, file, indent=2, ensure_ascii=False)
                    
                    successful = output_data['successful_matches']
                    total = output_data['total_sentences']
                    print(f"\nüéâ Successfully created {output_file}")
                    print(f"üìä Results: {successful}/{total} sentences matched with timestamps")
                    
                    # Print summary
                    print("\nüìù Summary:")
                    for item in sentence_transcriptions[:5]:  # Show first 5
                        if item['start'] is not None:
                            print(f"  {item['id']}: {item['start']}s - {item['end']}s")
                        else:
                            print(f"  {item['id']}: No timestamps found")
                    
                    if len(sentence_transcriptions) > 5:
                        print(f"  ... and {len(sentence_transcriptions) - 5} more sentences")
                        
                except Exception as e:
                    print(f"Error saving output file: {e}")
                    exit(1)
            else:
                print("No sentence transcriptions were created")
                exit(1)
        
        if __name__ == "__main__":
            main()
        EOF
        
    - name: Commit and push results
      env:
        GITHUB_TOKEN: ${{ secrets.GH_PAT }}
      run: |
        git config --local user.email "intellectra9@outlook.com"
        git config --local user.name "intellectra9"
        
        # Set up authentication
        git remote set-url origin https://intellectra9:${{ secrets.GH_PAT }}@github.com/intellectra9/intellect.git
        
        if [[ -f "sentence_transcriptions.json" ]]; then
          git add sentence_transcriptions.json
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "ü§ñ Auto-generate sentence transcriptions
            
            - Generated sentence-level timestamps from word-level transcription
            - Processed $(jq '.total_sentences' sentence_transcriptions.json) sentences
            - Successfully matched $(jq '.successful_matches' sentence_transcriptions.json) sentences with timestamps"
            
            git push origin HEAD:${{ github.ref_name }}
            echo "‚úÖ Sentence transcriptions committed and pushed!"
          fi
        else
          echo "‚ùå sentence_transcriptions.json was not created"
          exit 1
        fi
        
    - name: Upload artifact
      uses: actions/upload-artifact@v3
      with:
        name: sentence-transcriptions
        path: sentence_transcriptions.json
